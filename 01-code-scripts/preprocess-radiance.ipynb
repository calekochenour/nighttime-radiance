{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess VNP46A1 Nighttime Radiance Data\n",
    "\n",
    "This notebook provides a workflow to preprocess NASA VNP46A1 nighttime radiance data. The workflow reads HDF5 (.h5) files into arrays, masks pixels for clouds and sensor problems, and exports the preprocessed arrays to GeoTiff files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links:\n",
    "\n",
    "* https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/VNP46A1/\n",
    "* https://viirsland.gsfc.nasa.gov/PDF/VIIRS_BlackMarble_UserGuide.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download HDF5 files taken care of by `download_laads_order.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File naming convention:\n",
    "\n",
    "VNP46A1.AYYYYDDD.hXXvYY.CCC.YYYYDDDHHMMSS.h5\n",
    "\n",
    "* VNP46A1 = Short-name\n",
    "* AYYYYDDD = Acquisition Year and Day of Year\n",
    "* hXXvYY = Tile Identifier (horizontalXXverticalYY)\n",
    "* CCC = Collection Version\n",
    "* YYYYDDDHHMMSS = Production Date â€“ Year, Day, Hour, Minute, Second\n",
    "* h5 = Data Format (HDF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bands of interest (table from pages 12-13 of handbook)\n",
    "\n",
    "| Scientific Dataset          | Units             | Description            | Bit Types               | Fill Value | Valid Range | Scale Factor | Offset |\n",
    "|:-----------------------------|:-------------------|:------------------------|:-------------------------|:------------|:-------------|:--------------|:--------|\n",
    "| DNB_At_Sensor_Radiance_500m | nW_per_cm2_per_sr | At-sensor DNB radiance | 16-bit unsigned integer | 65535      | 0 - 65534   | 0.1          | 0.0    |\n",
    "| QF_Cloud_Mask               | Unitless          | Cloud mask status      | 16-bit unsigned integer | 65535      | 0 - 65534   | N/A          | N/A    |\n",
    "| QF_DNB                      | Unitless          | DNB_quality flag       | 16-bit unsigned integer | 65535      | 0 - 65534   | N/A          | N/A    |\n",
    "| UTC_Time                    | Decimal hours     | UTC Time               | 32-bit floating point   | -999.9     | 0 - 24      | 1.0          | 0.0    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get Georeferencing info not using GDAL? extract bounds from file using netCDF4 package?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Scripts:\n",
    "\n",
    "https://git.earthdata.nasa.gov/projects/LPDUR/repos/nasa-viirs/browse/scripts\n",
    "\n",
    "https://git.earthdata.nasa.gov/projects/LPDUR/repos/nasa-viirs/browse/scripts/VIIRS_HDF5toGeoTIFF.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "\n",
    "* Mask stack for no data\n",
    "* Mask stack for QA (cloud, snow, shadow, etc.)\n",
    "* Scale stack\n",
    "* Fill stack\n",
    "    * Have different fill values for different layers?\n",
    "    * And/or have fill values that are of the same `dtype` as the original layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking Criteria:\n",
    "\n",
    "* mask where DNB_At_Sensor_Radiance_500m == 65535\n",
    "* mask where DNB_At_Sensor_Radiance_500m > 65534\n",
    "* mask where DNB_At_Sensor_Radiance_500m < 0\n",
    "* mask where QF_Cloud_Mask == 2 (Probably Cloudy)\n",
    "* mask where QF_Cloud_Mask == 3 (Confident Cloudy)\n",
    "* mask where QF_DNB != 0 (0 = no problems, any other number means some kind of issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:04.156577Z",
     "start_time": "2020-10-24T17:09:03.966279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Load Notebook formatter\\n%load_ext nb_black\\n# %reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"# Load Notebook formatter\\n%load_ext nb_black\\n# %reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Notebook formatter\n",
    "%load_ext nb_black\n",
    "# %reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:08.188804Z",
     "start_time": "2020-10-24T17:09:04.223897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Import packages\\nimport os\\nimport re\\nimport glob\\nimport warnings\\nimport numpy as np\\nimport numpy.ma as ma\\nimport rasterio as rio\\nfrom rasterio.transform import from_origin\\nimport radiance as rd\";\n",
       "                var nbb_formatted_code = \"# Import packages\\nimport os\\nimport re\\nimport glob\\nimport warnings\\nimport numpy as np\\nimport numpy.ma as ma\\nimport rasterio as rio\\nfrom rasterio.transform import from_origin\\nimport radiance as rd\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import rasterio as rio\n",
    "from rasterio.transform import from_origin\n",
    "import radiance as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:08.282554Z",
     "start_time": "2020-10-24T17:09:08.272582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Set options\\n# sns.set(font_scale=1.5, style=\\\"whitegrid\\\")\\nwarnings.simplefilter(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# Set options\\n# sns.set(font_scale=1.5, style=\\\"whitegrid\\\")\\nwarnings.simplefilter(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set options\n",
    "# sns.set(font_scale=1.5, style=\"whitegrid\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:08.364335Z",
     "start_time": "2020-10-24T17:09:08.352368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\PSU\\08-covid19-remote-sensing-fusion\\00-git-repos\\nighttime-radiance\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Set working directory\\nos.chdir(\\\"..\\\")\\nprint(f\\\"Working directory: {os.getcwd()}\\\")\";\n",
       "                var nbb_formatted_code = \"# Set working directory\\nos.chdir(\\\"..\\\")\\nprint(f\\\"Working directory: {os.getcwd()}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set working directory\n",
    "os.chdir(\"..\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:08.460845Z",
     "start_time": "2020-10-24T17:09:08.439901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def extract_qa_bits(qa_band, start_bit, end_bit):\\n    \\\"\\\"\\\"Extracts the QA bitmask values for a specified\\n    bitmask (starting and ending bit).\\n\\n    Parameters\\n    ----------\\n    qa_band : numpy array\\n        Array containing the raw QA values (base-2) for all bitmasks.\\n\\n    start_bit : int\\n        First bit in the bitmask.\\n\\n    end_bit : int\\n        Last bit in the bitmask.\\n\\n    Returns\\n    -------\\n    qa_values : numpy array\\n        Array containing the extracted QA values (base-10) for the bitmask.\\n        \\n    Example\\n    -------\\n        >>>\\n        >>>\\n        >>>\\n        >>>\\n    \\\"\\\"\\\"\\n    # Initialize QA bit string/pattern to check QA band against\\n    qa_bits = 0\\n\\n    # Add each specified QA bit flag value/string/pattern\\n    #  to the QA bits to check/extract\\n    for bit in range(start_bit, end_bit + 1):\\n        qa_bits += bit ** 2\\n\\n    # Check QA band against specified QA bits to see what\\n    #  QA flag values are set\\n    qa_flags_set = qa_band & qa_bits\\n\\n    # Get base-10 value that matches bitmask documentation\\n    #  (0 or 1 for single bit,  0-3 or 0-N for multiple bits)\\n    qa_values = qa_flags_set >> start_bit\\n\\n    return qa_values\";\n",
       "                var nbb_formatted_code = \"def extract_qa_bits(qa_band, start_bit, end_bit):\\n    \\\"\\\"\\\"Extracts the QA bitmask values for a specified\\n    bitmask (starting and ending bit).\\n\\n    Parameters\\n    ----------\\n    qa_band : numpy array\\n        Array containing the raw QA values (base-2) for all bitmasks.\\n\\n    start_bit : int\\n        First bit in the bitmask.\\n\\n    end_bit : int\\n        Last bit in the bitmask.\\n\\n    Returns\\n    -------\\n    qa_values : numpy array\\n        Array containing the extracted QA values (base-10) for the bitmask.\\n        \\n    Example\\n    -------\\n        >>>\\n        >>>\\n        >>>\\n        >>>\\n    \\\"\\\"\\\"\\n    # Initialize QA bit string/pattern to check QA band against\\n    qa_bits = 0\\n\\n    # Add each specified QA bit flag value/string/pattern\\n    #  to the QA bits to check/extract\\n    for bit in range(start_bit, end_bit + 1):\\n        qa_bits += bit ** 2\\n\\n    # Check QA band against specified QA bits to see what\\n    #  QA flag values are set\\n    qa_flags_set = qa_band & qa_bits\\n\\n    # Get base-10 value that matches bitmask documentation\\n    #  (0 or 1 for single bit,  0-3 or 0-N for multiple bits)\\n    qa_values = qa_flags_set >> start_bit\\n\\n    return qa_values\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_qa_bits(qa_band, start_bit, end_bit):\n",
    "    \"\"\"Extracts the QA bitmask values for a specified\n",
    "    bitmask (starting and ending bit).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    qa_band : numpy array\n",
    "        Array containing the raw QA values (base-2) for all bitmasks.\n",
    "\n",
    "    start_bit : int\n",
    "        First bit in the bitmask.\n",
    "\n",
    "    end_bit : int\n",
    "        Last bit in the bitmask.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    qa_values : numpy array\n",
    "        Array containing the extracted QA values (base-10) for the bitmask.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "        >>>\n",
    "        >>>\n",
    "        >>>\n",
    "        >>>\n",
    "    \"\"\"\n",
    "    # Initialize QA bit string/pattern to check QA band against\n",
    "    qa_bits = 0\n",
    "\n",
    "    # Add each specified QA bit flag value/string/pattern\n",
    "    #  to the QA bits to check/extract\n",
    "    for bit in range(start_bit, end_bit + 1):\n",
    "        qa_bits += bit ** 2\n",
    "\n",
    "    # Check QA band against specified QA bits to see what\n",
    "    #  QA flag values are set\n",
    "    qa_flags_set = qa_band & qa_bits\n",
    "\n",
    "    # Get base-10 value that matches bitmask documentation\n",
    "    #  (0 or 1 for single bit,  0-3 or 0-N for multiple bits)\n",
    "    qa_values = qa_flags_set >> start_bit\n",
    "\n",
    "    return qa_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:08.614434Z",
     "start_time": "2020-10-24T17:09:08.580526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def create_transform_vnp46a1(hdf5):\\n    \\\"\\\"\\\"Creates a geographic transform for a VNP46A1 HDF5 file, \\n    based on longitude bounds, latitude bounds, and cell size.\\n    \\n    Parameters\\n    ----------\\n    hdf5 : str\\n        Path to an existsing VNP46A1 HDF5 file.\\n    \\n    Returns\\n    -------\\n    transform : affine.Affine object\\n    \\n    Example\\n    -------\\n        >>>\\n        >>>\\n        >>>\\n        >>>\\n    \\\"\\\"\\\"\\n    # Extract bounding box from top-level dataset\\n    with rio.open(hdf5) as dataset:\\n        longitude_min = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_WestBoundingCoord\\\"]\\n        )\\n        longitude_max = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_EastBoundingCoord\\\"]\\n        )\\n        latitude_min = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_SouthBoundingCoord\\\"]\\n        )\\n        latitude_max = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_NorthBoundingCoord\\\"]\\n        )\\n\\n        # Extract number of row and columns from first\\n        #  Science Data Set (subdataset/band)\\n        with rio.open(dataset.subdatasets[0]) as science_data_set:\\n            num_rows, num_columns = (\\n                science_data_set.meta.get(\\\"height\\\"),\\n                science_data_set.meta.get(\\\"width\\\"),\\n            )\\n\\n    # Define transform (top-left corner, cell size)\\n    transform = from_origin(\\n        longitude_min,\\n        latitude_max,\\n        (longitude_max - longitude_min) / num_columns,\\n        (latitude_max - latitude_min) / num_rows,\\n    )\\n\\n    return transform\";\n",
       "                var nbb_formatted_code = \"def create_transform_vnp46a1(hdf5):\\n    \\\"\\\"\\\"Creates a geographic transform for a VNP46A1 HDF5 file, \\n    based on longitude bounds, latitude bounds, and cell size.\\n    \\n    Parameters\\n    ----------\\n    hdf5 : str\\n        Path to an existsing VNP46A1 HDF5 file.\\n    \\n    Returns\\n    -------\\n    transform : affine.Affine object\\n    \\n    Example\\n    -------\\n        >>>\\n        >>>\\n        >>>\\n        >>>\\n    \\\"\\\"\\\"\\n    # Extract bounding box from top-level dataset\\n    with rio.open(hdf5) as dataset:\\n        longitude_min = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_WestBoundingCoord\\\"]\\n        )\\n        longitude_max = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_EastBoundingCoord\\\"]\\n        )\\n        latitude_min = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_SouthBoundingCoord\\\"]\\n        )\\n        latitude_max = int(\\n            dataset.tags()[\\\"HDFEOS_GRIDS_VNP_Grid_DNB_NorthBoundingCoord\\\"]\\n        )\\n\\n        # Extract number of row and columns from first\\n        #  Science Data Set (subdataset/band)\\n        with rio.open(dataset.subdatasets[0]) as science_data_set:\\n            num_rows, num_columns = (\\n                science_data_set.meta.get(\\\"height\\\"),\\n                science_data_set.meta.get(\\\"width\\\"),\\n            )\\n\\n    # Define transform (top-left corner, cell size)\\n    transform = from_origin(\\n        longitude_min,\\n        latitude_max,\\n        (longitude_max - longitude_min) / num_columns,\\n        (latitude_max - latitude_min) / num_rows,\\n    )\\n\\n    return transform\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_transform_vnp46a1(hdf5):\n",
    "    \"\"\"Creates a geographic transform for a VNP46A1 HDF5 file, \n",
    "    based on longitude bounds, latitude bounds, and cell size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf5 : str\n",
    "        Path to an existsing VNP46A1 HDF5 file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transform : affine.Affine object\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "        >>>\n",
    "        >>>\n",
    "        >>>\n",
    "        >>>\n",
    "    \"\"\"\n",
    "    # Extract bounding box from top-level dataset\n",
    "    with rio.open(hdf5) as dataset:\n",
    "        longitude_min = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_WestBoundingCoord\"]\n",
    "        )\n",
    "        longitude_max = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_EastBoundingCoord\"]\n",
    "        )\n",
    "        latitude_min = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_SouthBoundingCoord\"]\n",
    "        )\n",
    "        latitude_max = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_NorthBoundingCoord\"]\n",
    "        )\n",
    "\n",
    "        # Extract number of row and columns from first\n",
    "        #  Science Data Set (subdataset/band)\n",
    "        with rio.open(dataset.subdatasets[0]) as science_data_set:\n",
    "            num_rows, num_columns = (\n",
    "                science_data_set.meta.get(\"height\"),\n",
    "                science_data_set.meta.get(\"width\"),\n",
    "            )\n",
    "\n",
    "    # Define transform (top-left corner, cell size)\n",
    "    transform = from_origin(\n",
    "        longitude_min,\n",
    "        latitude_max,\n",
    "        (longitude_max - longitude_min) / num_columns,\n",
    "        (latitude_max - latitude_min) / num_rows,\n",
    "    )\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "* Download HDF files\n",
    "* Load HDF files\n",
    "* Read radiance band into array\n",
    "* Read cloud mask data into array \n",
    "* Read other QA data into array\n",
    "* Create stack of bands to use\n",
    "* Clip stack\n",
    "* Mask for DNB band quality\n",
    "* Mask for clouds\n",
    "* Mask radiance based on QA/cloud mask flags\n",
    "* Set no data/masked to 0\n",
    "* Set data outside bounds to 0\n",
    "* Store in dictionary based on year/month/day/time\n",
    "* Get metadata\n",
    "* Export radiance to GeoTiff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:10.820429Z",
     "start_time": "2020-10-24T17:09:10.805470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 HDF files to preprocess.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# Get HDF files into list\\nhdf_files = glob.glob(\\n    os.path.join(\\\"02-raw-data\\\", \\\"hdf\\\", \\\"south-korea\\\", \\\"*.h5\\\")\\n)\\nprint(f\\\"There are {len(hdf_files)} HDF files to preprocess.\\\")\";\n",
       "                var nbb_formatted_code = \"# Get HDF files into list\\nhdf_files = glob.glob(\\n    os.path.join(\\\"02-raw-data\\\", \\\"hdf\\\", \\\"south-korea\\\", \\\"*.h5\\\")\\n)\\nprint(f\\\"There are {len(hdf_files)} HDF files to preprocess.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get HDF files into list\n",
    "hdf_files = glob.glob(\n",
    "    os.path.join(\"02-raw-data\", \"hdf\", \"south-korea\", \"*.h5\")\n",
    ")\n",
    "print(f\"There are {len(hdf_files)} HDF files to preprocess.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:13.902962Z",
     "start_time": "2020-10-24T17:09:13.893979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# def extract_science_data_sets(hdf5_path):\\n#     \\\"\\\"\\\"Extracts the DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, and QF_DNB\\n#     science data sets (SDS) from a NASA VNP46A1 HDF5 file.\\n    \\n#     Parameters\\n#     ----------\\n#     hdf5_path : str\\n#         Path to the VNP46A1 HDF5 (.h5) file. \\n    \\n#     Returns\\n#     -------\\n#     tuple\\n    \\n#         dnb_at_sensor_radiance : numpy array\\n#            Array containing at-sensor radiance values.       \\n       \\n#         qf_dnb : numpy array\\n#             Array containing quality flag values for the at-sensor radiance\\n#             (sensor-related effects).    \\n        \\n#         qf_cloud_mask\\n#             Array containing the quality flag values for environment-related\\n#             effects (clouds, shadows, land/water).\\n    \\n#     Example\\n#     -------\\n#         >>>\\n#         >>>\\n#         >>>    \\n#     \\\"\\\"\\\"\\n#     # Open top-level dataset\\n#     with rio.open(hdf5_path) as dataset:\\n#         # Loop through subdatasets (Science Data Sets)\\n#         for science_data_set in dataset.subdatasets:\\n#             # Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB\\n#             if re.search(\\\"DNB_At_Sensor_Radiance_500m$\\\", science_data_set):\\n#                 with rio.open(science_data_set) as source:\\n#                     dnb_at_sensor_radiance = source.read(1)\\n#             if re.search(\\\"QF_Cloud_Mask$\\\", science_data_set):\\n#                 with rio.open(science_data_set) as source:\\n#                     qf_cloud_mask = source.read(1)\\n#             if re.search(\\\"QF_DNB$\\\", science_data_set):\\n#                 with rio.open(science_data_set) as source:\\n#                     qf_dnb = source.read(1)\\n                    \\n#     return dnb_at_sensor_radiance, qf\";\n",
       "                var nbb_formatted_code = \"# def extract_science_data_sets(hdf5_path):\\n#     \\\"\\\"\\\"Extracts the DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, and QF_DNB\\n#     science data sets (SDS) from a NASA VNP46A1 HDF5 file.\\n\\n#     Parameters\\n#     ----------\\n#     hdf5_path : str\\n#         Path to the VNP46A1 HDF5 (.h5) file.\\n\\n#     Returns\\n#     -------\\n#     tuple\\n\\n#         dnb_at_sensor_radiance : numpy array\\n#            Array containing at-sensor radiance values.\\n\\n#         qf_dnb : numpy array\\n#             Array containing quality flag values for the at-sensor radiance\\n#             (sensor-related effects).\\n\\n#         qf_cloud_mask\\n#             Array containing the quality flag values for environment-related\\n#             effects (clouds, shadows, land/water).\\n\\n#     Example\\n#     -------\\n#         >>>\\n#         >>>\\n#         >>>\\n#     \\\"\\\"\\\"\\n#     # Open top-level dataset\\n#     with rio.open(hdf5_path) as dataset:\\n#         # Loop through subdatasets (Science Data Sets)\\n#         for science_data_set in dataset.subdatasets:\\n#             # Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB\\n#             if re.search(\\\"DNB_At_Sensor_Radiance_500m$\\\", science_data_set):\\n#                 with rio.open(science_data_set) as source:\\n#                     dnb_at_sensor_radiance = source.read(1)\\n#             if re.search(\\\"QF_Cloud_Mask$\\\", science_data_set):\\n#                 with rio.open(science_data_set) as source:\\n#                     qf_cloud_mask = source.read(1)\\n#             if re.search(\\\"QF_DNB$\\\", science_data_set):\\n#                 with rio.open(science_data_set) as source:\\n#                     qf_dnb = source.read(1)\\n\\n#     return dnb_at_sensor_radiance, qf\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def extract_science_data_sets(hdf5_path):\n",
    "#     \"\"\"Extracts the DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, and QF_DNB\n",
    "#     science data sets (SDS) from a NASA VNP46A1 HDF5 file.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     hdf5_path : str\n",
    "#         Path to the VNP46A1 HDF5 (.h5) file.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     tuple\n",
    "\n",
    "#         dnb_at_sensor_radiance : numpy array\n",
    "#            Array containing at-sensor radiance values.\n",
    "\n",
    "#         qf_dnb : numpy array\n",
    "#             Array containing quality flag values for the at-sensor radiance\n",
    "#             (sensor-related effects).\n",
    "\n",
    "#         qf_cloud_mask\n",
    "#             Array containing the quality flag values for environment-related\n",
    "#             effects (clouds, shadows, land/water).\n",
    "\n",
    "#     Example\n",
    "#     -------\n",
    "#         >>>\n",
    "#         >>>\n",
    "#         >>>\n",
    "#     \"\"\"\n",
    "#     # Open top-level dataset\n",
    "#     with rio.open(hdf5_path) as dataset:\n",
    "#         # Loop through subdatasets (Science Data Sets)\n",
    "#         for science_data_set in dataset.subdatasets:\n",
    "#             # Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB\n",
    "#             if re.search(\"DNB_At_Sensor_Radiance_500m$\", science_data_set):\n",
    "#                 with rio.open(science_data_set) as source:\n",
    "#                     dnb_at_sensor_radiance = source.read(1)\n",
    "#             if re.search(\"QF_Cloud_Mask$\", science_data_set):\n",
    "#                 with rio.open(science_data_set) as source:\n",
    "#                     qf_cloud_mask = source.read(1)\n",
    "#             if re.search(\"QF_DNB$\", science_data_set):\n",
    "#                 with rio.open(science_data_set) as source:\n",
    "#                     qf_dnb = source.read(1)\n",
    "\n",
    "#     return dnb_at_sensor_radiance, qf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:16.849580Z",
     "start_time": "2020-10-24T17:09:16.806689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"def extract_band_vnp46a1(hdf5_path, band_name):\\n    \\\"\\\"\\\"Extracts the specified band (Science Data Set) from a NASA VNP46A1 HDF5\\n    file.\\n    \\n    Available Science Data Sets include:\\n    \\n    BrightnessTemperature_M12\\n    Moon_Illumination_Fraction\\n    Moon_Phase_Angle\\n    QF_Cloud_Mask\\n    QF_DNB\\n    QF_VIIRS_M10\\n    QF_VIIRS_M11\\n    QF_VIIRS_M12\\n    QF_VIIRS_M13\\n    QF_VIIRS_M15\\n    QF_VIIRS_M16\\n    BrightnessTemperature_M13\\n    Radiance_M10\\n    Radiance_M11\\n    Sensor_Azimuth\\n    Sensor_Zenith\\n    Solar_Azimuth\\n    Solar_Zenith\\n    UTC_Time\\n    BrightnessTemperature_M15\\n    BrightnessTemperature_M16\\n    DNB_At_Sensor_Radiance_500m\\n    Glint_Angle\\n    Granule\\n    Lunar_Azimuth\\n    Lunar_Zenith\\n    \\n    Parameters\\n    ----------\\n    hdf5_path : str\\n        Path to the VNP46A1 HDF5 (.h5) file. \\n        \\n    band_name : str\\n        Name of the band (Science Data Set) to be extracted. Must be an exact\\n        match to an available Science Data Set.     \\n    \\n    Returns\\n    -------\\n    band : numpy array\\n        Array containing the data for the specified band (Science Data Set).\\n    \\n    Example\\n    -------\\n        >>> qf_cloud_mask = extract_band_vnp46a1(\\n        ...     hdf5='VNP46A1.A2020001.h30v05.001.2020004003738.h5',\\n        ...     band='QF_Cloud_Mask'\\n        ... )\\n        >>> type(qf_cloud_mask)\\n        numpy.ndarray\\n    \\\"\\\"\\\"\\n    # Raise error for invalid band name\\n    band_names = [\\n        \\\"BrightnessTemperature_M12\\\",\\n        \\\"Moon_Illumination_Fraction\\\",\\n        \\\"Moon_Phase_Angle\\\",\\n        \\\"QF_Cloud_Mask\\\",\\n        \\\"QF_DNB\\\",\\n        \\\"QF_VIIRS_M10\\\",\\n        \\\"QF_VIIRS_M11\\\",\\n        \\\"QF_VIIRS_M12\\\",\\n        \\\"QF_VIIRS_M13\\\",\\n        \\\"QF_VIIRS_M15\\\",\\n        \\\"QF_VIIRS_M16\\\",\\n        \\\"BrightnessTemperature_M13\\\",\\n        \\\"Radiance_M10\\\",\\n        \\\"Radiance_M11\\\",\\n        \\\"Sensor_Azimuth\\\",\\n        \\\"Sensor_Zenith\\\",\\n        \\\"Solar_Azimuth\\\",\\n        \\\"Solar_Zenith\\\",\\n        \\\"UTC_Time\\\",\\n        \\\"BrightnessTemperature_M15\\\",\\n        \\\"BrightnessTemperature_M16\\\",\\n        \\\"DNB_At_Sensor_Radiance_500m\\\",\\n        \\\"Glint_Angle\\\",\\n        \\\"Granule\\\",\\n        \\\"Lunar_Azimuth\\\",\\n        \\\"Lunar_Zenith\\\",\\n    ]\\n    if band_name not in band_names:\\n        raise ValueError(\\n            f\\\"Invalid band name. Must be one of the following: {band_names}\\\"\\n        )\\n\\n    # Open top-level dataset, loop through Science Data Sets (subdatasets),\\n    #  and extract specified band\\n    with rio.open(hdf5_path) as dataset:\\n        for science_data_set in dataset.subdatasets:\\n            if re.search(f\\\"{band_name}$\\\", science_data_set):\\n                with rio.open(science_data_set) as src:\\n                    band = src.read(1)\\n\\n    return band\";\n",
       "                var nbb_formatted_code = \"def extract_band_vnp46a1(hdf5_path, band_name):\\n    \\\"\\\"\\\"Extracts the specified band (Science Data Set) from a NASA VNP46A1 HDF5\\n    file.\\n    \\n    Available Science Data Sets include:\\n    \\n    BrightnessTemperature_M12\\n    Moon_Illumination_Fraction\\n    Moon_Phase_Angle\\n    QF_Cloud_Mask\\n    QF_DNB\\n    QF_VIIRS_M10\\n    QF_VIIRS_M11\\n    QF_VIIRS_M12\\n    QF_VIIRS_M13\\n    QF_VIIRS_M15\\n    QF_VIIRS_M16\\n    BrightnessTemperature_M13\\n    Radiance_M10\\n    Radiance_M11\\n    Sensor_Azimuth\\n    Sensor_Zenith\\n    Solar_Azimuth\\n    Solar_Zenith\\n    UTC_Time\\n    BrightnessTemperature_M15\\n    BrightnessTemperature_M16\\n    DNB_At_Sensor_Radiance_500m\\n    Glint_Angle\\n    Granule\\n    Lunar_Azimuth\\n    Lunar_Zenith\\n    \\n    Parameters\\n    ----------\\n    hdf5_path : str\\n        Path to the VNP46A1 HDF5 (.h5) file. \\n        \\n    band_name : str\\n        Name of the band (Science Data Set) to be extracted. Must be an exact\\n        match to an available Science Data Set.     \\n    \\n    Returns\\n    -------\\n    band : numpy array\\n        Array containing the data for the specified band (Science Data Set).\\n    \\n    Example\\n    -------\\n        >>> qf_cloud_mask = extract_band_vnp46a1(\\n        ...     hdf5='VNP46A1.A2020001.h30v05.001.2020004003738.h5',\\n        ...     band='QF_Cloud_Mask'\\n        ... )\\n        >>> type(qf_cloud_mask)\\n        numpy.ndarray\\n    \\\"\\\"\\\"\\n    # Raise error for invalid band name\\n    band_names = [\\n        \\\"BrightnessTemperature_M12\\\",\\n        \\\"Moon_Illumination_Fraction\\\",\\n        \\\"Moon_Phase_Angle\\\",\\n        \\\"QF_Cloud_Mask\\\",\\n        \\\"QF_DNB\\\",\\n        \\\"QF_VIIRS_M10\\\",\\n        \\\"QF_VIIRS_M11\\\",\\n        \\\"QF_VIIRS_M12\\\",\\n        \\\"QF_VIIRS_M13\\\",\\n        \\\"QF_VIIRS_M15\\\",\\n        \\\"QF_VIIRS_M16\\\",\\n        \\\"BrightnessTemperature_M13\\\",\\n        \\\"Radiance_M10\\\",\\n        \\\"Radiance_M11\\\",\\n        \\\"Sensor_Azimuth\\\",\\n        \\\"Sensor_Zenith\\\",\\n        \\\"Solar_Azimuth\\\",\\n        \\\"Solar_Zenith\\\",\\n        \\\"UTC_Time\\\",\\n        \\\"BrightnessTemperature_M15\\\",\\n        \\\"BrightnessTemperature_M16\\\",\\n        \\\"DNB_At_Sensor_Radiance_500m\\\",\\n        \\\"Glint_Angle\\\",\\n        \\\"Granule\\\",\\n        \\\"Lunar_Azimuth\\\",\\n        \\\"Lunar_Zenith\\\",\\n    ]\\n    if band_name not in band_names:\\n        raise ValueError(\\n            f\\\"Invalid band name. Must be one of the following: {band_names}\\\"\\n        )\\n\\n    # Open top-level dataset, loop through Science Data Sets (subdatasets),\\n    #  and extract specified band\\n    with rio.open(hdf5_path) as dataset:\\n        for science_data_set in dataset.subdatasets:\\n            if re.search(f\\\"{band_name}$\\\", science_data_set):\\n                with rio.open(science_data_set) as src:\\n                    band = src.read(1)\\n\\n    return band\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_band_vnp46a1(hdf5_path, band_name):\n",
    "    \"\"\"Extracts the specified band (Science Data Set) from a NASA VNP46A1 HDF5\n",
    "    file.\n",
    "    \n",
    "    Available Science Data Sets include:\n",
    "    \n",
    "    BrightnessTemperature_M12\n",
    "    Moon_Illumination_Fraction\n",
    "    Moon_Phase_Angle\n",
    "    QF_Cloud_Mask\n",
    "    QF_DNB\n",
    "    QF_VIIRS_M10\n",
    "    QF_VIIRS_M11\n",
    "    QF_VIIRS_M12\n",
    "    QF_VIIRS_M13\n",
    "    QF_VIIRS_M15\n",
    "    QF_VIIRS_M16\n",
    "    BrightnessTemperature_M13\n",
    "    Radiance_M10\n",
    "    Radiance_M11\n",
    "    Sensor_Azimuth\n",
    "    Sensor_Zenith\n",
    "    Solar_Azimuth\n",
    "    Solar_Zenith\n",
    "    UTC_Time\n",
    "    BrightnessTemperature_M15\n",
    "    BrightnessTemperature_M16\n",
    "    DNB_At_Sensor_Radiance_500m\n",
    "    Glint_Angle\n",
    "    Granule\n",
    "    Lunar_Azimuth\n",
    "    Lunar_Zenith\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf5_path : str\n",
    "        Path to the VNP46A1 HDF5 (.h5) file. \n",
    "        \n",
    "    band_name : str\n",
    "        Name of the band (Science Data Set) to be extracted. Must be an exact\n",
    "        match to an available Science Data Set.     \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    band : numpy array\n",
    "        Array containing the data for the specified band (Science Data Set).\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "        >>> qf_cloud_mask = extract_band_vnp46a1(\n",
    "        ...     hdf5='VNP46A1.A2020001.h30v05.001.2020004003738.h5',\n",
    "        ...     band='QF_Cloud_Mask'\n",
    "        ... )\n",
    "        >>> type(qf_cloud_mask)\n",
    "        numpy.ndarray\n",
    "    \"\"\"\n",
    "    # Raise error for invalid band name\n",
    "    band_names = [\n",
    "        \"BrightnessTemperature_M12\",\n",
    "        \"Moon_Illumination_Fraction\",\n",
    "        \"Moon_Phase_Angle\",\n",
    "        \"QF_Cloud_Mask\",\n",
    "        \"QF_DNB\",\n",
    "        \"QF_VIIRS_M10\",\n",
    "        \"QF_VIIRS_M11\",\n",
    "        \"QF_VIIRS_M12\",\n",
    "        \"QF_VIIRS_M13\",\n",
    "        \"QF_VIIRS_M15\",\n",
    "        \"QF_VIIRS_M16\",\n",
    "        \"BrightnessTemperature_M13\",\n",
    "        \"Radiance_M10\",\n",
    "        \"Radiance_M11\",\n",
    "        \"Sensor_Azimuth\",\n",
    "        \"Sensor_Zenith\",\n",
    "        \"Solar_Azimuth\",\n",
    "        \"Solar_Zenith\",\n",
    "        \"UTC_Time\",\n",
    "        \"BrightnessTemperature_M15\",\n",
    "        \"BrightnessTemperature_M16\",\n",
    "        \"DNB_At_Sensor_Radiance_500m\",\n",
    "        \"Glint_Angle\",\n",
    "        \"Granule\",\n",
    "        \"Lunar_Azimuth\",\n",
    "        \"Lunar_Zenith\",\n",
    "    ]\n",
    "    if band_name not in band_names:\n",
    "        raise ValueError(\n",
    "            f\"Invalid band name. Must be one of the following: {band_names}\"\n",
    "        )\n",
    "\n",
    "    # Open top-level dataset, loop through Science Data Sets (subdatasets),\n",
    "    #  and extract specified band\n",
    "    with rio.open(hdf5_path) as dataset:\n",
    "        for science_data_set in dataset.subdatasets:\n",
    "            if re.search(f\"{band_name}$\", science_data_set):\n",
    "                with rio.open(science_data_set) as src:\n",
    "                    band = src.read(1)\n",
    "\n",
    "    return band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T16:14:38.103721Z",
     "start_time": "2020-10-24T16:14:37.985809Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_band_vnp46a1(hdf5_path=hdf_files[1], band_name=\"QF_Cloud_Mask\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:31.029894Z",
     "start_time": "2020-10-24T17:09:31.021932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# # Loop through each HDF file\\n# for hdf in [hdf_files[0]]:\\n#     # Open top-level dataset\\n#     with rio.open(hdf) as dataset:\\n#         # Loop through subdatasets (Science Data Sets)\\n#         for science_data_set in dataset.subdatasets:\\n#             # Get name of Science Data Set (SDS)\\n#             science_data_set_name = science_data_set.split(os.sep)[-1].split(\\n#                 \\\"/\\\"\\n#             )[-1]\\n#             print(science_data_set_name)\";\n",
       "                var nbb_formatted_code = \"# # Loop through each HDF file\\n# for hdf in [hdf_files[0]]:\\n#     # Open top-level dataset\\n#     with rio.open(hdf) as dataset:\\n#         # Loop through subdatasets (Science Data Sets)\\n#         for science_data_set in dataset.subdatasets:\\n#             # Get name of Science Data Set (SDS)\\n#             science_data_set_name = science_data_set.split(os.sep)[-1].split(\\n#                 \\\"/\\\"\\n#             )[-1]\\n#             print(science_data_set_name)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Loop through each HDF file\n",
    "# for hdf in [hdf_files[0]]:\n",
    "#     # Open top-level dataset\n",
    "#     with rio.open(hdf) as dataset:\n",
    "#         # Loop through subdatasets (Science Data Sets)\n",
    "#         for science_data_set in dataset.subdatasets:\n",
    "#             # Get name of Science Data Set (SDS)\n",
    "#             science_data_set_name = science_data_set.split(os.sep)[-1].split(\n",
    "#                 \"/\"\n",
    "#             )[-1]\n",
    "#             print(science_data_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:09:53.794822Z",
     "start_time": "2020-10-24T17:09:53.707057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"def preprocess_vnp46a1(hdf5_path, output_folder):\\n    \\\"\\\"\\\"Preprocessed a NASA VNP46A1 HDF5 (.h5 file)\\n    \\n    Preprocessing steps include masking data for fill values, clouds, and \\n    sensor problems, filling masked values, and exporting data to a GeoTiff.\\n    \\n    Parameters\\n    ----------\\n    hdf5_path : str\\n        Path to the VNP46A1 HDF5 (.h5) file to be preprocessed. \\n    \\n    output_folder : str\\n        Path to the folder where the preprocessed file will be exported to.\\n    \\n    Returns\\n    -------\\n    message : str\\n        Indication of preprocessing completion status (success or failure).\\n    \\n    Example\\n    -------\\n        >>>\\n        >>>\\n        >>>\\n        >>>\\n    \\\"\\\"\\\"\\n    # Preprocess VNP46A1 HDF5 file\\n    print(f\\\"Started preprocessing: {os.path.basename(hdf5_path)}\\\")\\n    try:\\n        print(\\\"Extracting bands...\\\")\\n        # Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB\\n        dnb_at_sensor_radiance = extract_band_vnp46a1(\\n            hdf5_path=hdf, band_name=\\\"DNB_At_Sensor_Radiance_500m\\\"\\n        )\\n        qf_cloud_mask = extract_band_vnp46a1(\\n            hdf5_path=hdf, band_name=\\\"QF_Cloud_Mask\\\"\\n        )\\n        qf_dnb = extract_band_vnp46a1(hdf5_path=hdf, band_name=\\\"QF_DNB\\\")\\n\\n        print(\\\"Applying scale factor...\\\")\\n        # Apply scale factor to radiance values\\n        dnb_at_sensor_radiance_scaled = (\\n            dnb_at_sensor_radiance.astype(\\\"float\\\") * 0.1\\n        )\\n\\n        print(\\\"Masking for fill values...\\\")\\n        # Mask radiance for fill value (DNB_At_Sensor_Radiance_500m == 65535)\\n        masked_for_fill_value = ma.masked_where(\\n            dnb_at_sensor_radiance_scaled == 6553.5,\\n            dnb_at_sensor_radiance_scaled,\\n            copy=True,\\n        )\\n\\n        print(\\\"Masking for clouds...\\\")\\n        # Extract QF_Cloud_Mask bits 6-7 (Cloud Detection Results &\\n        #  Confidence Indicator)\\n        cloud_detection_bitmask = extract_qa_bits(\\n            qa_band=qf_cloud_mask, start_bit=6, end_bit=7\\n        )\\n\\n        # Mask radiance for 'probably cloudy' (cloud_detection_bitmask == 2)\\n        masked_for_probably_cloudy = ma.masked_where(\\n            cloud_detection_bitmask == 2, masked_for_fill_value, copy=True\\n        )\\n\\n        # Mask radiance for 'confident cloudy' (cloud_detection_bitmask == 3)\\n        masked_for_confident_cloudy = ma.masked_where(\\n            cloud_detection_bitmask == 3, masked_for_probably_cloudy, copy=True\\n        )\\n\\n        print(\\\"Masking for sensor problems...\\\")\\n        # Mask radiance for sensor problems (QF_DNB != 0)\\n        #  (0 = no problems, any number > 0 means some kind of issue)\\n        masked_for_sensor_problems = ma.masked_where(\\n            qf_dnb > 0, masked_for_confident_cloudy, copy=True\\n        )\\n\\n        print(\\\"Filling masked values...\\\")\\n        # Set fill value to np.nan and fill masked values\\n        ma.set_fill_value(masked_for_sensor_problems, np.nan)\\n        filled_data = masked_for_sensor_problems.filled()\\n\\n        print(\\\"Creating metadata...\\\")\\n        # Create metadata (for export)\\n        metadata = rd.create_metadata(\\n            array=filled_data,\\n            transform=create_transform_vnp46a1(hdf),\\n            driver=\\\"GTiff\\\",\\n            nodata=np.nan,\\n            count=1,\\n            crs=\\\"epsg:4326\\\",\\n        )\\n\\n        print(\\\"Exporting to GeoTiff...\\\")\\n        # Export masked array to GeoTiff (no data set to np.nan in export)\\n        rd.export_array(\\n            array=filled_data,\\n            output_path=os.path.join(\\n                output_folder,\\n                f\\\"{os.path.basename(hdf)[:-3].lower().replace('.', '-')}.tif\\\",\\n            ),\\n            metadata=metadata,\\n        )\\n    except Exception as error:\\n        message = print(f\\\"Preprocessing failed: {error}\\\")\\n    else:\\n        message = print(\\n            f\\\"Completed preprocessing: {os.path.basename(hdf5_path)}\\\\n\\\"\\n        )\\n\\n    return message\";\n",
       "                var nbb_formatted_code = \"def preprocess_vnp46a1(hdf5_path, output_folder):\\n    \\\"\\\"\\\"Preprocessed a NASA VNP46A1 HDF5 (.h5 file)\\n    \\n    Preprocessing steps include masking data for fill values, clouds, and \\n    sensor problems, filling masked values, and exporting data to a GeoTiff.\\n    \\n    Parameters\\n    ----------\\n    hdf5_path : str\\n        Path to the VNP46A1 HDF5 (.h5) file to be preprocessed. \\n    \\n    output_folder : str\\n        Path to the folder where the preprocessed file will be exported to.\\n    \\n    Returns\\n    -------\\n    message : str\\n        Indication of preprocessing completion status (success or failure).\\n    \\n    Example\\n    -------\\n        >>>\\n        >>>\\n        >>>\\n        >>>\\n    \\\"\\\"\\\"\\n    # Preprocess VNP46A1 HDF5 file\\n    print(f\\\"Started preprocessing: {os.path.basename(hdf5_path)}\\\")\\n    try:\\n        print(\\\"Extracting bands...\\\")\\n        # Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB\\n        dnb_at_sensor_radiance = extract_band_vnp46a1(\\n            hdf5_path=hdf, band_name=\\\"DNB_At_Sensor_Radiance_500m\\\"\\n        )\\n        qf_cloud_mask = extract_band_vnp46a1(\\n            hdf5_path=hdf, band_name=\\\"QF_Cloud_Mask\\\"\\n        )\\n        qf_dnb = extract_band_vnp46a1(hdf5_path=hdf, band_name=\\\"QF_DNB\\\")\\n\\n        print(\\\"Applying scale factor...\\\")\\n        # Apply scale factor to radiance values\\n        dnb_at_sensor_radiance_scaled = (\\n            dnb_at_sensor_radiance.astype(\\\"float\\\") * 0.1\\n        )\\n\\n        print(\\\"Masking for fill values...\\\")\\n        # Mask radiance for fill value (DNB_At_Sensor_Radiance_500m == 65535)\\n        masked_for_fill_value = ma.masked_where(\\n            dnb_at_sensor_radiance_scaled == 6553.5,\\n            dnb_at_sensor_radiance_scaled,\\n            copy=True,\\n        )\\n\\n        print(\\\"Masking for clouds...\\\")\\n        # Extract QF_Cloud_Mask bits 6-7 (Cloud Detection Results &\\n        #  Confidence Indicator)\\n        cloud_detection_bitmask = extract_qa_bits(\\n            qa_band=qf_cloud_mask, start_bit=6, end_bit=7\\n        )\\n\\n        # Mask radiance for 'probably cloudy' (cloud_detection_bitmask == 2)\\n        masked_for_probably_cloudy = ma.masked_where(\\n            cloud_detection_bitmask == 2, masked_for_fill_value, copy=True\\n        )\\n\\n        # Mask radiance for 'confident cloudy' (cloud_detection_bitmask == 3)\\n        masked_for_confident_cloudy = ma.masked_where(\\n            cloud_detection_bitmask == 3, masked_for_probably_cloudy, copy=True\\n        )\\n\\n        print(\\\"Masking for sensor problems...\\\")\\n        # Mask radiance for sensor problems (QF_DNB != 0)\\n        #  (0 = no problems, any number > 0 means some kind of issue)\\n        masked_for_sensor_problems = ma.masked_where(\\n            qf_dnb > 0, masked_for_confident_cloudy, copy=True\\n        )\\n\\n        print(\\\"Filling masked values...\\\")\\n        # Set fill value to np.nan and fill masked values\\n        ma.set_fill_value(masked_for_sensor_problems, np.nan)\\n        filled_data = masked_for_sensor_problems.filled()\\n\\n        print(\\\"Creating metadata...\\\")\\n        # Create metadata (for export)\\n        metadata = rd.create_metadata(\\n            array=filled_data,\\n            transform=create_transform_vnp46a1(hdf),\\n            driver=\\\"GTiff\\\",\\n            nodata=np.nan,\\n            count=1,\\n            crs=\\\"epsg:4326\\\",\\n        )\\n\\n        print(\\\"Exporting to GeoTiff...\\\")\\n        # Export masked array to GeoTiff (no data set to np.nan in export)\\n        rd.export_array(\\n            array=filled_data,\\n            output_path=os.path.join(\\n                output_folder,\\n                f\\\"{os.path.basename(hdf)[:-3].lower().replace('.', '-')}.tif\\\",\\n            ),\\n            metadata=metadata,\\n        )\\n    except Exception as error:\\n        message = print(f\\\"Preprocessing failed: {error}\\\")\\n    else:\\n        message = print(\\n            f\\\"Completed preprocessing: {os.path.basename(hdf5_path)}\\\\n\\\"\\n        )\\n\\n    return message\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_vnp46a1(hdf5_path, output_folder):\n",
    "    \"\"\"Preprocessed a NASA VNP46A1 HDF5 (.h5 file)\n",
    "    \n",
    "    Preprocessing steps include masking data for fill values, clouds, and \n",
    "    sensor problems, filling masked values, and exporting data to a GeoTiff.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf5_path : str\n",
    "        Path to the VNP46A1 HDF5 (.h5) file to be preprocessed. \n",
    "    \n",
    "    output_folder : str\n",
    "        Path to the folder where the preprocessed file will be exported to.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    message : str\n",
    "        Indication of preprocessing completion status (success or failure).\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "        >>>\n",
    "        >>>\n",
    "        >>>\n",
    "        >>>\n",
    "    \"\"\"\n",
    "    # Preprocess VNP46A1 HDF5 file\n",
    "    print(f\"Started preprocessing: {os.path.basename(hdf5_path)}\")\n",
    "    try:\n",
    "        print(\"Extracting bands...\")\n",
    "        # Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB\n",
    "        dnb_at_sensor_radiance = extract_band_vnp46a1(\n",
    "            hdf5_path=hdf, band_name=\"DNB_At_Sensor_Radiance_500m\"\n",
    "        )\n",
    "        qf_cloud_mask = extract_band_vnp46a1(\n",
    "            hdf5_path=hdf, band_name=\"QF_Cloud_Mask\"\n",
    "        )\n",
    "        qf_dnb = extract_band_vnp46a1(hdf5_path=hdf, band_name=\"QF_DNB\")\n",
    "\n",
    "        print(\"Applying scale factor...\")\n",
    "        # Apply scale factor to radiance values\n",
    "        dnb_at_sensor_radiance_scaled = (\n",
    "            dnb_at_sensor_radiance.astype(\"float\") * 0.1\n",
    "        )\n",
    "\n",
    "        print(\"Masking for fill values...\")\n",
    "        # Mask radiance for fill value (DNB_At_Sensor_Radiance_500m == 65535)\n",
    "        masked_for_fill_value = ma.masked_where(\n",
    "            dnb_at_sensor_radiance_scaled == 6553.5,\n",
    "            dnb_at_sensor_radiance_scaled,\n",
    "            copy=True,\n",
    "        )\n",
    "\n",
    "        print(\"Masking for clouds...\")\n",
    "        # Extract QF_Cloud_Mask bits 6-7 (Cloud Detection Results &\n",
    "        #  Confidence Indicator)\n",
    "        cloud_detection_bitmask = extract_qa_bits(\n",
    "            qa_band=qf_cloud_mask, start_bit=6, end_bit=7\n",
    "        )\n",
    "\n",
    "        # Mask radiance for 'probably cloudy' (cloud_detection_bitmask == 2)\n",
    "        masked_for_probably_cloudy = ma.masked_where(\n",
    "            cloud_detection_bitmask == 2, masked_for_fill_value, copy=True\n",
    "        )\n",
    "\n",
    "        # Mask radiance for 'confident cloudy' (cloud_detection_bitmask == 3)\n",
    "        masked_for_confident_cloudy = ma.masked_where(\n",
    "            cloud_detection_bitmask == 3, masked_for_probably_cloudy, copy=True\n",
    "        )\n",
    "\n",
    "        print(\"Masking for sensor problems...\")\n",
    "        # Mask radiance for sensor problems (QF_DNB != 0)\n",
    "        #  (0 = no problems, any number > 0 means some kind of issue)\n",
    "        masked_for_sensor_problems = ma.masked_where(\n",
    "            qf_dnb > 0, masked_for_confident_cloudy, copy=True\n",
    "        )\n",
    "\n",
    "        print(\"Filling masked values...\")\n",
    "        # Set fill value to np.nan and fill masked values\n",
    "        ma.set_fill_value(masked_for_sensor_problems, np.nan)\n",
    "        filled_data = masked_for_sensor_problems.filled()\n",
    "\n",
    "        print(\"Creating metadata...\")\n",
    "        # Create metadata (for export)\n",
    "        metadata = rd.create_metadata(\n",
    "            array=filled_data,\n",
    "            transform=create_transform_vnp46a1(hdf),\n",
    "            driver=\"GTiff\",\n",
    "            nodata=np.nan,\n",
    "            count=1,\n",
    "            crs=\"epsg:4326\",\n",
    "        )\n",
    "\n",
    "        print(\"Exporting to GeoTiff...\")\n",
    "        # Export masked array to GeoTiff (no data set to np.nan in export)\n",
    "        rd.export_array(\n",
    "            array=filled_data,\n",
    "            output_path=os.path.join(\n",
    "                output_folder,\n",
    "                f\"{os.path.basename(hdf)[:-3].lower().replace('.', '-')}.tif\",\n",
    "            ),\n",
    "            metadata=metadata,\n",
    "        )\n",
    "    except Exception as error:\n",
    "        message = print(f\"Preprocessing failed: {error}\")\n",
    "    else:\n",
    "        message = print(\n",
    "            f\"Completed preprocessing: {os.path.basename(hdf5_path)}\\n\"\n",
    "        )\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract bands\n",
    "* Mask for fill values\n",
    "* Mask for clouds\n",
    "* Mask for sensor problems\n",
    "* Fill masked values\n",
    "* Create transform\n",
    "* Create metadata\n",
    "* Export array to GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T17:10:03.834189Z",
     "start_time": "2020-10-24T17:09:56.322553Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started preprocessing: VNP46A1.A2020001.h30v05.001.2020004003738.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020001-h30v05-001-2020004003738.tif\n",
      "Completed preprocessing: VNP46A1.A2020001.h30v05.001.2020004003738.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020001.h31v05.001.2020004003841.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020001-h31v05-001-2020004003841.tif\n",
      "Completed preprocessing: VNP46A1.A2020001.h31v05.001.2020004003841.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020002.h30v05.001.2020004085319.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020002-h30v05-001-2020004085319.tif\n",
      "Completed preprocessing: VNP46A1.A2020002.h30v05.001.2020004085319.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020002.h31v05.001.2020004084330.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020002-h31v05-001-2020004084330.tif\n",
      "Completed preprocessing: VNP46A1.A2020002.h31v05.001.2020004084330.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020003.h30v05.001.2020007073916.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020003-h30v05-001-2020007073916.tif\n",
      "Completed preprocessing: VNP46A1.A2020003.h30v05.001.2020007073916.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020003.h31v05.001.2020007082529.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020003-h31v05-001-2020007082529.tif\n",
      "Completed preprocessing: VNP46A1.A2020003.h31v05.001.2020007082529.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020004.h30v05.001.2020007184958.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020004-h30v05-001-2020007184958.tif\n",
      "Completed preprocessing: VNP46A1.A2020004.h30v05.001.2020007184958.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020004.h31v05.001.2020007184404.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020004-h31v05-001-2020007184404.tif\n",
      "Completed preprocessing: VNP46A1.A2020004.h31v05.001.2020007184404.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020005.h30v05.001.2020007214129.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020005-h30v05-001-2020007214129.tif\n",
      "Completed preprocessing: VNP46A1.A2020005.h30v05.001.2020007214129.h5\n",
      "\n",
      "Started preprocessing: VNP46A1.A2020005.h31v05.001.2020007211820.h5\n",
      "Extracting bands...\n",
      "Applying scale factor...\n",
      "Masking for fill values...\n",
      "Masking for clouds...\n",
      "Masking for sensor problems...\n",
      "Filling masked values...\n",
      "Creating metadata...\n",
      "Exporting to GeoTiff...\n",
      "Exported: vnp46a1-a2020005-h31v05-001-2020007211820.tif\n",
      "Completed preprocessing: VNP46A1.A2020005.h31v05.001.2020007211820.h5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# Preprocess each HDF file\\nfor hdf in hdf_files:\\n    preprocess_vnp46a1(\\n        hdf5_path=hdf,\\n        output_folder=os.path.join(\\n            \\\"03-processed-data\\\", \\\"raster\\\", \\\"south-korea\\\", \\\"vnp46a1-grid\\\"\\n        ),\\n    )\";\n",
       "                var nbb_formatted_code = \"# Preprocess each HDF file\\nfor hdf in hdf_files:\\n    preprocess_vnp46a1(\\n        hdf5_path=hdf,\\n        output_folder=os.path.join(\\n            \\\"03-processed-data\\\", \\\"raster\\\", \\\"south-korea\\\", \\\"vnp46a1-grid\\\"\\n        ),\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess each HDF file\n",
    "for hdf in hdf_files:\n",
    "    preprocess_vnp46a1(\n",
    "        hdf5_path=hdf,\n",
    "        output_folder=os.path.join(\n",
    "            \"03-processed-data\", \"raster\", \"south-korea\", \"vnp46a1-grid\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T16:51:57.037061Z",
     "start_time": "2020-10-24T16:51:48.008534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through each HDF file\n",
    "for hdf in hdf_files:\n",
    "    # Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB\n",
    "    dnb_at_sensor_radiance = extract_band_vnp46a1(\n",
    "        hdf5_path=hdf, band_name=\"DNB_At_Sensor_Radiance_500m\"\n",
    "    )\n",
    "    qf_cloud_mask = extract_band_vnp46a1(\n",
    "        hdf5_path=hdf, band_name=\"QF_Cloud_Mask\"\n",
    "    )\n",
    "    qf_dnb = extract_band_vnp46a1(hdf5_path=hdf, band_name=\"QF_DNB\")\n",
    "\n",
    "    # Apply scale factor to radiance values\n",
    "    dnb_at_sensor_radiance_scaled = (\n",
    "        dnb_at_sensor_radiance.astype(\"float\") * 0.1\n",
    "    )\n",
    "\n",
    "    # Mask radiance for fill value (DNB_At_Sensor_Radiance_500m == 65535)\n",
    "    masked_for_fill_value = ma.masked_where(\n",
    "        dnb_at_sensor_radiance_scaled == 6553.5,\n",
    "        dnb_at_sensor_radiance_scaled,\n",
    "        copy=True,\n",
    "    )\n",
    "\n",
    "    # Extract QF_Cloud_Mask bits 6-7 (Cloud Detection Results &\n",
    "    #  Confidence Indicator)\n",
    "    cloud_detection_bitmask = extract_qa_bits(\n",
    "        qa_band=qf_cloud_mask, start_bit=6, end_bit=7\n",
    "    )\n",
    "\n",
    "    # Mask radiance for 'probably cloudy' (cloud_detection_bitmask == 2)\n",
    "    masked_for_probably_cloudy = ma.masked_where(\n",
    "        cloud_detection_bitmask == 2, masked_for_fill_value, copy=True\n",
    "    )\n",
    "\n",
    "    # Mask radiance for 'confident cloudy' (cloud_detection_bitmask == 3)\n",
    "    masked_for_confident_cloudy = ma.masked_where(\n",
    "        cloud_detection_bitmask == 3, masked_for_probably_cloudy, copy=True\n",
    "    )\n",
    "\n",
    "    # Mask radiance for sensor problems (QF_DNB != 0)\n",
    "    #  (0 = no problems, any number > 0 means some kind of issue)\n",
    "    masked_for_sensor_problems = ma.masked_where(\n",
    "        qf_dnb > 0, masked_for_confident_cloudy, copy=True\n",
    "    )\n",
    "\n",
    "    # Set fill value to np.nan and fill masked values\n",
    "    ma.set_fill_value(masked_for_sensor_problems, np.nan)\n",
    "    filled_data = masked_for_sensor_problems.filled()\n",
    "\n",
    "    print(f\"Max scaled value: {np.nanmax(filled_data)}\")\n",
    "    print(f\"Mean scaled value: {np.nanmean(filled_data)}\")\n",
    "    print(f\"Median scaled value: {np.nanmedian(filled_data)}\")\n",
    "    print(\n",
    "        f\"Number of masked pixels: {np.count_nonzero(np.isnan(filled_data))}\\n\"\n",
    "    )\n",
    "\n",
    "    # Create metadata (for export)\n",
    "    metadata = rd.create_metadata(\n",
    "        array=filled_data,\n",
    "        transform=create_transform_vnp46a1(hdf),\n",
    "        driver=\"GTiff\",\n",
    "        nodata=np.nan,\n",
    "        count=1,\n",
    "        crs=\"epsg:4326\",\n",
    "    )\n",
    "\n",
    "    # Export masked array to GeoTiff (no data set to np.nan in export)\n",
    "    rd.export_array(\n",
    "        array=filled_data,\n",
    "        output_path=os.path.join(\n",
    "            \"03-processed-data\",\n",
    "            \"raster\",\n",
    "            \"south-korea\",\n",
    "            \"vnp46a1-grid\",\n",
    "            f\"{os.path.basename(hdf)[:-3].lower().replace('.', '-')}.tif\",\n",
    "        ),\n",
    "        metadata=metadata,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T16:08:33.082829Z",
     "start_time": "2020-10-24T16:08:25.509293Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop through each HDF file\n",
    "for hdf in hdf_files:\n",
    "    # Open top-level dataset\n",
    "    with rio.open(hdf) as dataset:\n",
    "        # Loop through subdatasets (Science Data Sets)\n",
    "        for science_data_set in dataset.subdatasets:\n",
    "            #             # Get name of Science Data Set (SDS)\n",
    "            #             science_data_set_name = science_data_set.split(os.sep)[-1].split(\n",
    "            #                 \"/\"\n",
    "            #             )[-1]\n",
    "            #             print(science_data_set)\n",
    "            # Extract Science Data Sets of interest\n",
    "            #  (DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB)\n",
    "            if re.search(\"DNB_At_Sensor_Radiance_500m$\", science_data_set):\n",
    "                with rio.open(science_data_set) as source:\n",
    "                    dnb_at_sensor_radiance = source.read(1)\n",
    "            if re.search(\"QF_Cloud_Mask$\", science_data_set):\n",
    "                with rio.open(science_data_set) as source:\n",
    "                    qf_cloud_mask = source.read(1)\n",
    "            if re.search(\"QF_DNB$\", science_data_set):\n",
    "                with rio.open(science_data_set) as source:\n",
    "                    qf_dnb = source.read(1)\n",
    "\n",
    "    # Convert at-sensor radiance array to type float\n",
    "    dnb_at_sensor_radiance_float = dnb_at_sensor_radiance.astype(\"float\")\n",
    "\n",
    "    # Apply scale factor to radiance values\n",
    "    dnb_at_sensor_radiance_scaled = dnb_at_sensor_radiance_float * 0.1\n",
    "\n",
    "    # Mask radiance for fill value (DNB_At_Sensor_Radiance_500m == 65535)\n",
    "    masked_for_fill_value = ma.masked_where(\n",
    "        dnb_at_sensor_radiance_scaled == 6553.5,\n",
    "        dnb_at_sensor_radiance_scaled,\n",
    "        copy=True,\n",
    "    )\n",
    "\n",
    "    # Mask radiance for outside valid range (DNB_At_Sensor_Radiance_500m > 65534)\n",
    "    # Mask radiance for outside valid range (DNB_At_Sensor_Radiance_500m < 0)\n",
    "\n",
    "    # Extract QF_Cloud_Mask bits 6-7 (Cloud Detection Results & Confidence Indicator)\n",
    "    cloud_detection_bitmask = extract_qa_bits(\n",
    "        qa_band=qf_cloud_mask, start_bit=6, end_bit=7\n",
    "    )\n",
    "\n",
    "    # Mask radiance for 'probably cloudy' (cloud_detection_bitmask == 2)\n",
    "    masked_for_probably_cloudy = ma.masked_where(\n",
    "        cloud_detection_bitmask == 2, masked_for_fill_value, copy=True\n",
    "    )\n",
    "\n",
    "    # Mask radiance for 'confident cloudy' (cloud_detection_bitmask == 3)\n",
    "    masked_for_confident_cloudy = ma.masked_where(\n",
    "        cloud_detection_bitmask == 3, masked_for_probably_cloudy, copy=True\n",
    "    )\n",
    "\n",
    "    # Mask radiance for sensor problems (QF_DNB != 0)\n",
    "    #  (0 = no problems, any number > 0 means some kind of issue)\n",
    "    masked_for_sensor_problems = ma.masked_where(\n",
    "        qf_dnb > 0, masked_for_confident_cloudy, copy=True\n",
    "    )\n",
    "\n",
    "    #     print(masked_for_sensor_problems)\n",
    "\n",
    "    # Extract QF_Cloud_Mask bit 9 (Cirrus detection)\n",
    "    #     cirrus_detection_bitmask = extract_qa_bits(\n",
    "    #         qa_band=qf_cloud_mask, start_bit=9, end_bit=9\n",
    "    #     )\n",
    "\n",
    "    # Mask radiance for Cirrus detection (cirrus_detection_bitmask == 1)\n",
    "    #     masked_for_cirrus = ma.masked_where(\n",
    "    #         cirrus_detection_bitmask == 1, masked_for_sensor_problems, copy=True\n",
    "    #     )\n",
    "\n",
    "    # Fill masked values with np.nan or 0 - NEED TO FIX THIS VALUE OR DTYPE\n",
    "    # Set fill value to np.nan\n",
    "    #     ma.set_fill_value(masked_for_sensor_problems, 0)\n",
    "    #     ma.set_fill_value(masked_for_sensor_problems, -99990.0)\n",
    "    ma.set_fill_value(masked_for_sensor_problems, np.nan)\n",
    "\n",
    "    # Fill masked values with np.nan\n",
    "    filled_data = masked_for_sensor_problems.filled()\n",
    "    #     filled_data = masked_for_cirrus.filled()\n",
    "\n",
    "    # Count the number of NaN values\n",
    "    #     np.count_nonzero(np.isnan(filled_data))\n",
    "    # np.nan is np.NaN is np.NAN ==> True\n",
    "\n",
    "    #     print(f\"Max value after masking\")\n",
    "    # Apply scale factor to data\n",
    "    #     scaled_array = apply_scale_factor(filled_array, scale_factor=scale_factor)\n",
    "    # Apply scale factor to data\n",
    "    #     scaled_data = filled_data * 0.1\n",
    "\n",
    "    print(f\"Max scaled value: {np.nanmax(filled_data)}\")\n",
    "    print(f\"Mean scaled value: {np.nanmean(filled_data)}\")\n",
    "    print(f\"Median scaled value: {np.nanmedian(filled_data)}\")\n",
    "    print(\n",
    "        f\"Number of masked pixels: {np.count_nonzero(np.isnan(filled_data))}\\n\"\n",
    "    )\n",
    "\n",
    "    # Set file export name\n",
    "    export_name = f\"{os.path.basename(hdf)[:-3].lower().replace('.', '-')}.tif\"\n",
    "\n",
    "    # Create transform (for export)\n",
    "    transform = create_transform_vnp46a1(hdf)\n",
    "\n",
    "    # Create metadata (for export)\n",
    "    metadata = rd.create_metadata(\n",
    "        array=filled_data,\n",
    "        transform=transform,\n",
    "        driver=\"GTiff\",\n",
    "        #         nodata=-9999.0,  # NEED TO CHANGE THIS\n",
    "        #         nodata=0,  # NEED TO CHANGE THIS\n",
    "        nodata=np.nan,  # NEED TO CHANGE THIS\n",
    "        count=1,\n",
    "        crs=\"epsg:4326\",\n",
    "    )\n",
    "\n",
    "    # Export masked array to GeoTiff (no data set to np.nan in export)\n",
    "#     rd.export_array(\n",
    "#         array=scaled_data,\n",
    "#         output_path=os.path.join(\n",
    "#             \"03-processed-data\",\n",
    "#             \"raster\",\n",
    "#             \"south-korea\",\n",
    "#             \"vnp46a1-grid\",\n",
    "#             export_name,\n",
    "#         ),\n",
    "#         metadata=metadata,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T15:18:07.689697Z",
     "start_time": "2020-10-23T15:18:00.368278Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define list of bands used in preprocessing\n",
    "required_bands = [\n",
    "    \"DNB_At_Sensor_Radiance_500m\",\n",
    "    \"QF_Cloud_Mask\",\n",
    "    \"QF_DNB\",\n",
    "    \"UTC_Time\",\n",
    "]\n",
    "\n",
    "# Loop through each HDF file\n",
    "for hdf in hdf_files:\n",
    "    # Open top-level dataset\n",
    "    with rio.open(hdf) as dataset:\n",
    "        # Loop through subdatasets (Science Data Sets)\n",
    "        for science_data_set in dataset.subdatasets:\n",
    "            #             # Get name of Science Data Set (SDS)\n",
    "            #             science_data_set_name = science_data_set.split(os.sep)[-1].split(\n",
    "            #                 \"/\"\n",
    "            #             )[-1]\n",
    "            #             print(science_data_set)\n",
    "            # Extract Science Data Sets of interest\n",
    "            #  (DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, QF_DNB)\n",
    "            if re.search(\"DNB_At_Sensor_Radiance_500m$\", science_data_set):\n",
    "                with rio.open(science_data_set) as source:\n",
    "                    dnb_at_sensor_radiance = source.read(1)\n",
    "            if re.search(\"QF_Cloud_Mask$\", science_data_set):\n",
    "                with rio.open(science_data_set) as source:\n",
    "                    qf_cloud_mask = source.read(1)\n",
    "            if re.search(\"QF_DNB$\", science_data_set):\n",
    "                with rio.open(science_data_set) as source:\n",
    "                    qf_dnb = source.read(1)\n",
    "\n",
    "    # Apply scale factor to radiance values data\n",
    "    dnb_at_sensor_radiance_scaled = dnb_at_sensor_radiance * 0.1\n",
    "                    \n",
    "    # Mask radiance for fill value (DNB_At_Sensor_Radiance_500m == 65535)\n",
    "    masked_for_fill_value = ma.masked_where(\n",
    "        dnb_at_sensor_radiance == 65535, dnb_at_sensor_radiance, copy=True\n",
    "    ).astype(\"int32\")\n",
    "\n",
    "    # Mask radiance for outside valid range (DNB_At_Sensor_Radiance_500m > 65534)\n",
    "    # Mask radiance for outside valid range (DNB_At_Sensor_Radiance_500m < 0)\n",
    "\n",
    "    # Extract QF_Cloud_Mask bits 6-7 (Cloud Detection Results & Confidence Indicator)\n",
    "    cloud_detection_bitmask = extract_qa_bits(\n",
    "        qa_band=qf_cloud_mask, start_bit=6, end_bit=7\n",
    "    )\n",
    "\n",
    "    # Mask radiance for 'probably cloudy' (cloud_detection_bitmask == 2)\n",
    "    masked_for_probably_cloudy = ma.masked_where(\n",
    "        cloud_detection_bitmask == 2, masked_for_fill_value, copy=True\n",
    "    )\n",
    "\n",
    "    # Mask radiance for 'confident cloudy' (cloud_detection_bitmask == 3)\n",
    "    masked_for_confident_cloudy = ma.masked_where(\n",
    "        cloud_detection_bitmask == 3, masked_for_probably_cloudy, copy=True\n",
    "    )\n",
    "\n",
    "    # Mask radiance for sensor problems (QF_DNB != 0)\n",
    "    #  (0 = no problems, any number > 0 means some kind of issue)\n",
    "    masked_for_sensor_problems = ma.masked_where(\n",
    "        qf_dnb > 0, masked_for_confident_cloudy, copy=True\n",
    "    )\n",
    "\n",
    "    print(masked_for_sensor_problems)\n",
    "\n",
    "    # Extract QF_Cloud_Mask bit 9 (Cirrus detection)\n",
    "    #     cirrus_detection_bitmask = extract_qa_bits(\n",
    "    #         qa_band=qf_cloud_mask, start_bit=9, end_bit=9\n",
    "    #     )\n",
    "\n",
    "    # Mask radiance for Cirrus detection (cirrus_detection_bitmask == 1)\n",
    "    #     masked_for_cirrus = ma.masked_where(\n",
    "    #         cirrus_detection_bitmask == 1, masked_for_sensor_problems, copy=True\n",
    "    #     )\n",
    "\n",
    "    # Fill masked values with np.nan or 0 - NEED TO FIX THIS VALUE OR DTYPE\n",
    "    # Change fill value to NaN\n",
    "    #     ma.set_fill_value(masked_for_sensor_problems, 0)\n",
    "    ma.set_fill_value(masked_for_sensor_problems, -99990.0)\n",
    "    #     ma.set_fill_value(masked_for_sensor_problems, np.nan)\n",
    "\n",
    "    # Fill masked values with NaN\n",
    "    filled_data = masked_for_sensor_problems.filled()\n",
    "    #     filled_data = masked_for_cirrus.filled()\n",
    "\n",
    "    #     print(f\"Max value after masking\")\n",
    "    # Apply scale factor to data\n",
    "    #     scaled_array = apply_scale_factor(filled_array, scale_factor=scale_factor)\n",
    "    # Apply scale factor to data\n",
    "    scaled_data = filled_data * 0.1\n",
    "\n",
    "    print(f\"Max scaled value: {scaled_data.max()}\")\n",
    "    print(f\"Mean scaled value: {np.mean(scaled_data)}\")\n",
    "    print(f\"Median scaled value: {np.median(scaled_data)}\\n\")\n",
    "\n",
    "    # Set file export name\n",
    "    export_name = f\"{os.path.basename(hdf)[:-3].lower().replace('.', '-')}.tif\"\n",
    "\n",
    "    # Create transform (for export)\n",
    "    transform = create_transform_vnp46a1(hdf)\n",
    "\n",
    "    # Create metadata (for export)\n",
    "    metadata = rd.create_metadata(\n",
    "        array=scaled_data,\n",
    "        transform=transform,\n",
    "        driver=\"GTiff\",\n",
    "        nodata=-9999.0,  # NEED TO CHANGE THIS\n",
    "        #         nodata=0,  # NEED TO CHANGE THIS\n",
    "        #         nodata=np.nan, # NEED TO CHANGE THIS\n",
    "        count=1,\n",
    "        crs=\"epsg:4326\",\n",
    "    )\n",
    "\n",
    "    # Export masked array to GeoTiff (no data set to np.nan in export)\n",
    "    rd.export_array(\n",
    "        array=scaled_data,\n",
    "        output_path=os.path.join(\n",
    "            \"03-processed-data\",\n",
    "            \"raster\",\n",
    "            \"south-korea\",\n",
    "            \"vnp46a1-grid\",\n",
    "            export_name,\n",
    "        ),\n",
    "        metadata=metadata,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:56:04.512720Z",
     "start_time": "2020-10-21T21:56:04.504718Z"
    }
   },
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((scaled_data, scaled_data), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:49:46.905784Z",
     "start_time": "2020-10-21T21:49:46.878876Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = create_transform_vnp46a1(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:49:54.126828Z",
     "start_time": "2020-10-21T21:49:54.117854Z"
    }
   },
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:42:50.694875Z",
     "start_time": "2020-10-23T14:42:50.654978Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = create_transform_vnp46a1(hdf_files[1])\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:42:58.057261Z",
     "start_time": "2020-10-23T14:42:58.047268Z"
    }
   },
   "outputs": [],
   "source": [
    "type(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:49:19.882265Z",
     "start_time": "2020-10-21T21:49:19.874281Z"
    }
   },
   "outputs": [],
   "source": [
    "export_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:37:21.976538Z",
     "start_time": "2020-10-21T21:37:21.949585Z"
    }
   },
   "outputs": [],
   "source": [
    "max_value_index = np.unravel_index(\n",
    "    np.argmax(masked_for_sensor_problems, axis=None),\n",
    "    masked_for_sensor_problems.shape,\n",
    ")\n",
    "\n",
    "max_value_index\n",
    "\n",
    "# a[ind]\n",
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:27:24.530628Z",
     "start_time": "2020-10-21T21:27:23.801578Z"
    }
   },
   "outputs": [],
   "source": [
    "ep.plot_bands(masked_for_sensor_problems)\n",
    "# np.median(masked_for_sensor_problems)\n",
    "# np.argmax(masked_for_sensor_problems, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full geotiff\n",
    "# Crop to south korea boundary (get cropped image and metadata) es.crop_image\n",
    "# Export cropped array to geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:14:40.305896Z",
     "start_time": "2020-10-21T21:14:40.133331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract bitmasks for masking radiance data\n",
    "day_night_bitmask = extract_qa_bits(\n",
    "    qa_band=qf_cloud_mask, start_bit=0, end_bit=0\n",
    ")\n",
    "\n",
    "land_water_background_bitmask = extract_qa_bits(\n",
    "    qa_band=qf_cloud_mask, start_bit=1, end_bit=3\n",
    ")\n",
    "\n",
    "cloud_mask_quality_bitmask = extract_qa_bits(\n",
    "    qa_band=qf_cloud_mask, start_bit=4, end_bit=5\n",
    ")\n",
    "\n",
    "cloud_detection_bitmask = extract_qa_bits(\n",
    "    qa_band=qf_cloud_mask, start_bit=6, end_bit=7\n",
    ")\n",
    "\n",
    "shadow_detected_bitmask = extract_qa_bits(\n",
    "    qa_band=qf_cloud_mask, start_bit=8, end_bit=8\n",
    ")\n",
    "\n",
    "cirrus_detection_bitmask = extract_qa_bits(\n",
    "    qa_band=qf_cloud_mask, start_bit=9, end_bit=9\n",
    ")\n",
    "\n",
    "snow_ice_surface_bitmask = extract_qa_bits(\n",
    "    qa_band=qf_cloud_mask, start_bit=10, end_bit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:14:49.450809Z",
     "start_time": "2020-10-21T21:14:49.440834Z"
    }
   },
   "outputs": [],
   "source": [
    "day_night_bitmask[max_value_index] # 0 = Night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:15:17.303648Z",
     "start_time": "2020-10-21T21:15:17.294671Z"
    }
   },
   "outputs": [],
   "source": [
    "land_water_background_bitmask[max_value_index] # 3 = Sea Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:16:00.390493Z",
     "start_time": "2020-10-21T21:16:00.380516Z"
    }
   },
   "outputs": [],
   "source": [
    "cloud_mask_quality_bitmask[max_value_index] # 2 = Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:16:27.224176Z",
     "start_time": "2020-10-21T21:16:27.214234Z"
    }
   },
   "outputs": [],
   "source": [
    "cloud_detection_bitmask[max_value_index] # 0 = Confident Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:17:07.148167Z",
     "start_time": "2020-10-21T21:17:07.138192Z"
    }
   },
   "outputs": [],
   "source": [
    "shadow_detected_bitmask[max_value_index] # 0 = No Shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:17:59.022198Z",
     "start_time": "2020-10-21T21:17:59.011226Z"
    }
   },
   "outputs": [],
   "source": [
    "cirrus_detection_bitmask[max_value_index]  # 0 = No Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:18:08.682427Z",
     "start_time": "2020-10-21T21:18:08.672421Z"
    }
   },
   "outputs": [],
   "source": [
    "snow_ice_surface_bitmask[max_value_index] # 0 = No Snow/Ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:20:02.902895Z",
     "start_time": "2020-10-21T21:20:02.802158Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(qf_dnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T21:20:26.407695Z",
     "start_time": "2020-10-21T21:20:26.398747Z"
    }
   },
   "outputs": [],
   "source": [
    "qf_dnb[max_value_index] # 0 = No problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:12:01.673698Z",
     "start_time": "2020-10-20T20:12:01.663726Z"
    }
   },
   "outputs": [],
   "source": [
    "dnb_at_sensor_radiance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:12:05.997601Z",
     "start_time": "2020-10-20T20:12:05.988648Z"
    }
   },
   "outputs": [],
   "source": [
    "dnb_at_sensor_radiance_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:12:25.901583Z",
     "start_time": "2020-10-20T20:12:25.891608Z"
    }
   },
   "outputs": [],
   "source": [
    "dnb_at_sensor_radiance_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:17:19.121181Z",
     "start_time": "2020-10-20T20:17:19.096217Z"
    }
   },
   "outputs": [],
   "source": [
    "for hdf in [hdf_files[0]]:\n",
    "    # Open top-level dataset (has no geospatial information)\n",
    "    with rio.open(hdf) as dataset:\n",
    "        print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T13:58:49.845997Z",
     "start_time": "2020-10-21T13:58:49.744240Z"
    }
   },
   "outputs": [],
   "source": [
    "for hdf in [hdf_files[0]]:\n",
    "    print(hdf[\"HDFEO)\n",
    "    # Open top-level dataset (has no geospatial information)\n",
    "#     with rio.open(hdf) as dataset:\n",
    "#         print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:04:21.971950Z",
     "start_time": "2020-10-21T14:04:21.943028Z"
    }
   },
   "outputs": [],
   "source": [
    "with rio.open(hdf_files[0]) as dataset:\n",
    "    print(dataset)\n",
    "    hdf5_metadata = dataset.meta\n",
    "    crs = dataset.read_crs()\n",
    "    print(dataset.read_crs())  # crs is None\n",
    "    print(type(dataset.tags()))\n",
    "    print(\n",
    "        dataset.tags()\n",
    "    )  # ['NorthBoundingCoord']), EastBoundingCoord, WestBoundingCoord, SouthBoundingCoord\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:15:13.667636Z",
     "start_time": "2020-10-21T14:15:13.568927Z"
    }
   },
   "outputs": [],
   "source": [
    "with rio.open(hdf_files[0]) as dataset:\n",
    "    print(dataset)\n",
    "    hdf5_metadata = dataset.meta\n",
    "    crs = dataset.read_crs()\n",
    "    print(dataset.read_crs())  # crs is None\n",
    "    print(type(dataset.tags()))\n",
    "    #     print(\n",
    "    #         dataset.tags()\n",
    "    #     )  # ['NorthBoundingCoord']), EastBoundingCoord, WestBoundingCoord, SouthBoundingCoord\n",
    "    for key in dataset.tags():\n",
    "        #         if \"EastBounding\" in key:\n",
    "        print(f\"{key}: {dataset.tags()[key]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:35:35.804377Z",
     "start_time": "2020-10-21T14:35:35.768501Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_transform_vnp46a1(hdf5):\n",
    "    \"\"\"Creates a geographic transform for a VNP46A1 HDF5 file, \n",
    "    based on longitude bounds, latitude bounds, and cell size.\n",
    "    \"\"\"\n",
    "    # Extract bounding coordinates and number of rows/colums from top-level\n",
    "    with rio.open(hdf5) as dataset:\n",
    "        longitude_min = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_WestBoundingCoord\"]\n",
    "        )\n",
    "        longitude_max = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_EastBoundingCoord\"]\n",
    "        )\n",
    "        latitude_min = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_SouthBoundingCoord\"]\n",
    "        )\n",
    "        latitude_max = int(\n",
    "            dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_NorthBoundingCoord\"]\n",
    "        )\n",
    "\n",
    "        # Extract cell size from first band (science data set)\n",
    "        with rio.open(dataset.subdatasets[0]) as band:\n",
    "            num_rows, num_columns = (\n",
    "                band.meta.get(\"height\"),\n",
    "                band.meta.get(\"width\"),\n",
    "            )\n",
    "\n",
    "    # Define transform (top-left corner, cell size)\n",
    "    transform = from_origin(\n",
    "        longitude_min,\n",
    "        latitude_max,\n",
    "        (longitude_max - longitude_min) / num_columns,\n",
    "        (latitude_max - latitude_min) / num_rows,\n",
    "    )\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:36:00.759343Z",
     "start_time": "2020-10-21T14:36:00.727402Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = create_transform_vnp46a1(hdf_files[0])\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:22:19.631746Z",
     "start_time": "2020-10-21T14:22:19.578851Z"
    }
   },
   "outputs": [],
   "source": [
    "with rio.open(hdf_files[0]) as dataset:\n",
    "    longitude_min = int(\n",
    "        dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_WestBoundingCoord\"]\n",
    "    )\n",
    "    longitude_max = int(\n",
    "        dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_EastBoundingCoord\"]\n",
    "    )\n",
    "    latitude_min = int(\n",
    "        dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_SouthBoundingCoord\"]\n",
    "    )\n",
    "    latitude_max = int(\n",
    "        dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_NorthBoundingCoord\"]\n",
    "    )\n",
    "\n",
    "    # Extract cell size from first band (science data set)\n",
    "    with rio.open(dataset.subdatasets[0]) as band:\n",
    "        num_rows, num_columns = band.meta.get(\"height\"), band.meta.get(\"width\")\n",
    "    #         print(band.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:39:46.702787Z",
     "start_time": "2020-10-21T14:39:46.689788Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_image_date_vnp46a1(hdf5):\n",
    "    \"\"\"Returns the acquisition date of a VNP46A1 HDF5 file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf5 : str\n",
    "        Path to the VNP46A1 HDF5 file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    date : str\n",
    "        Acquisition date of the image, formatted as 'YYYY-MM-DD'.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "        >>> hdf5_file = \"VNP46A1.A2020001.h30v05.001.2020004003738.h5\"\n",
    "        >>> extract_image_date_vnp46a1(hdf5_file)\n",
    "        '2020-01-01'   \n",
    "    \"\"\"\n",
    "    # Open file and extract date\n",
    "    with rio.open(hdf5) as dataset:\n",
    "        date = dataset.tags()[\"HDFEOS_GRIDS_VNP_Grid_DNB_RangeBeginningDate\"]\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:39:51.823707Z",
     "start_time": "2020-10-21T14:39:51.801733Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_image_date_vnp46a1(hdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* QF_Cloud_Mask\n",
    "    * Bit 0 - Day/Night \t\n",
    "        * 0 (Night)\n",
    "        * 1 (Day)\n",
    "        \n",
    "    * Bits 1-3 - Land/Water Background\n",
    "        * 0 (Land & Desert)\n",
    "        * 1 (Land no Desert)\n",
    "        * 2 (Inland Water)\n",
    "        * 3 (Sea Water)\n",
    "        * 5 (Coastal)\n",
    "        \n",
    "    * Bits 4-5 - Cloud Mask Quality \t\n",
    "        * 0 (Poor)\n",
    "        * 1 (Low)\n",
    "        * 2 (Medium)\n",
    "        * 3 (High)\n",
    "    \n",
    "    * Bits 6-7 - Cloud Detection Results & Confidence Indicator \n",
    "        * 0 (Confident Clear)\n",
    "        * 1 (Probably Clear)\n",
    "        * 2 (Probably Cloudy)\n",
    "        * 3 (Confident Cloudy)\n",
    "\n",
    "    * Bit 8 - Shadow Detected\n",
    "        * 1 (Yes)\n",
    "        * 0 (No)\n",
    "\n",
    "    * Bit 9 - Cirrus Detection (IR) (BTM15 â€“BTM16) \t\n",
    "        * 1 (Cloud)\n",
    "        * 0 (No Cloud)\n",
    "\n",
    "    * Bit 10 - Snow/Ice Surface\n",
    "        * 1 (Snow/Ice)\n",
    "        * 0 (No Snow/Ice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QF_Cloud_Mask (base-2):\n",
    "\n",
    "| Bit | Flag Description Key                          | Interpretation                                                                            |\n",
    "|:-----|:-----------------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| 0   | Day/Night                                     | 0 = Night <br> 1 = Day                                                                         |\n",
    "| 1-3 | Land/Water Background                         | 000 = Land & Desert <br> 001 = Land no Desert <br> 010 = Inland Water <br> 011 = Sea Water <br> 101 = Coastal |\n",
    "| 4-5 | Cloud Mask Quality                            | 00 = Poor <br> 01 = Low <br> 10 = Medium <br> 11 = High                                                  |\n",
    "| 6-7 | Cloud Detection Results & Confidence Indicator | 00 = Confident Clear <br> 01 = Probably Clear <br> 10 = Probably Cloudy <br> 11 = Confident Cloudy     |\n",
    "| 8   | Shadow Detected                               | 1 = Yes <br> 0 = No                                                                            |\n",
    "| 9   | Cirrus Detection (IR) (BTM15 â€“BTM16)          | 1 = Cloud <br> 0 = No Cloud                                                                   |\n",
    "| 10  | Snow/Ice Surface                              | 1 = Snow/Ice <br> 0 = No Snow/Ice                                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QF_Cloud_Mask (base-10):\n",
    "\n",
    "| Bit | Flag Description Key                          | Interpretation                                                                            |\n",
    "|:-----|:-----------------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| 0   | Day/Night                                     | 0 = Night <br> 1 = Day                                                                         |\n",
    "| 1-3 | Land/Water Background                         | 0 = Land & Desert <br> 1 = Land no Desert <br> 2 = Inland Water <br> 3 = Sea Water <br> 5 = Coastal |\n",
    "| 4-5 | Cloud Mask Quality                            | 0 = Poor <br> 1 = Low <br> 2 = Medium <br> 3 = High                                                  |\n",
    "| 6-7 | Cloud Detection Results & Confidence Indicator | 0 = Confident Clear <br> 1 = Probably Clear <br> 2 = Probably Cloudy <br> 3 = Confident Cloudy     |\n",
    "| 8   | Shadow Detected                               | 1 = Yes <br> 0 = No                                                                            |\n",
    "| 9   | Cirrus Detection (IR) (BTM15 â€“BTM16)          | 1 = Cloud <br> 0 = No Cloud                                                                   |\n",
    "| 10  | Snow/Ice Surface                              | 1 = Snow/Ice <br> 0 = No Snow/Ice                                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QF_DNB:\n",
    "\n",
    "| SDS Layer | Flag Mask Values and Descriptions|\n",
    "|:-----------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| QF_DNB    | 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = Substitute_Cal<br>2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = Out_of_Range<br>4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = Saturation<br>8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = Temp_not_Nominal<br>16&nbsp;&nbsp;&nbsp;&nbsp; = Stray_Light<br>256&nbsp;&nbsp; = Bowtie_Deleted/Range_Bit<br>512&nbsp;&nbsp; = Missing_EV<br>1024 = Cal_Fail<br>2048 = Dead_Detector |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:49:24.236636Z",
     "start_time": "2020-10-21T14:49:24.217686Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_qa_bits(qa_band, start_bit, end_bit):\n",
    "    \"\"\"Extracts the QA bitmask values for a specified\n",
    "    bitmask (starting and ending bit).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    qa_band : numpy array\n",
    "        Array containing the raw QA values (base-2) for all bitmasks.\n",
    "\n",
    "    start_bit : int\n",
    "        First bit in the bitmask.\n",
    "\n",
    "    end_bit : int\n",
    "        Last bit in the bitmask.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    qa_values :  numpy array\n",
    "        Array containing the extracted QA values (base-10) for the bitmask.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize QA bit string/pattern to check QA band against\n",
    "    qa_bits = 0\n",
    "\n",
    "    # Add each specified QA bit flag value/string/pattern\n",
    "    #  to the QA bits to check/extract\n",
    "    for bit in range(start_bit, end_bit + 1):\n",
    "        qa_bits += bit ** 2\n",
    "\n",
    "    # Check QA band against specified QA bits to see what QA flag values are set\n",
    "    qa_flags_set = qa_band & qa_bits\n",
    "\n",
    "    # Get base-10 value that matches bitmask documentation\n",
    "    #  (0 or 1 for single bit,  0-3 or 0-N for multiple bits)\n",
    "    qa_values = qa_flags_set >> start_bit\n",
    "\n",
    "    # Return array of qa_values for specific bits\n",
    "    return qa_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:19:06.561986Z",
     "start_time": "2020-10-21T14:19:06.552015Z"
    }
   },
   "outputs": [],
   "source": [
    "latitude_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T14:16:35.600350Z",
     "start_time": "2020-10-21T14:16:35.591400Z"
    }
   },
   "outputs": [],
   "source": [
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_science_datasets(hdf5):\n",
    "    \"\"\"Stacks all 7 Science Data Sets into a single array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hdf5 : str\n",
    "        Path to the HDF5 file.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize list to store all science datasets\n",
    "    science_datasets = []\n",
    "\n",
    "    # Populate list with all science datasets\n",
    "    with rio.open(hdf5_file) as dataset:\n",
    "        for science_dataset in dataset.subdatasets:\n",
    "            with rio.open(science_dataset) as band:\n",
    "                science_datasets.append(band.read(1))\n",
    "\n",
    "    # Stack science datasets into single array            \n",
    "    science_datasets_stacked = np.stack(science_datasets)\n",
    "    \n",
    "    return science_datasets_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of bands used in preprocessing\n",
    "required_bands = [\n",
    "    \"DNB_At_Sensor_Radiance_500m\",\n",
    "    \"QF_Cloud_Mask\",\n",
    "    \"QF_DNB\",\n",
    "    \"UTC_Time\",\n",
    "]\n",
    "\n",
    "# Loop through each HDF file\n",
    "for hdf in [hdf_files[0]]:\n",
    "    # Open top-level dataset (has no geospatial information)\n",
    "    with rio.open(hdf) as dataset:\n",
    "        #         print(dataset.meta)\n",
    "        #         print(dataset.profile)\n",
    "        #         print(dataset.read_crs())\n",
    "        # Loop through subdatasets (bands)\n",
    "        for band in dataset.subdatasets:\n",
    "            band_name = band.split(os.sep)[-1].split(\"/\")[-1]\n",
    "            # print(f\"Band name: {band.split(os.sep)[-1].split('/')[-1]}\")\n",
    "            print(f\"Band name: {band_name}\")\n",
    "            if band_name == required_bands[0]:\n",
    "                with rio.open(band) as src:\n",
    "                    dnb_at_sensor_radiance = src.read(1)\n",
    "                    dnb_at_sensor_radiance_metadata = src.meta\n",
    "                    dnb_at_sensor_radiance_profile = src.profile\n",
    "#             qf_cloud_mask\n",
    "#             qf_dnb_\n",
    "#             utc_time\n",
    "\n",
    "#             if band_name in required_bands:\n",
    "#                 print(f\"Band name: {band_name}\")\n",
    "#                 print()\n",
    "#             print(band.meta)\n",
    "# Extract DNB_At_Sensor_Radiance_500m, QF_Cloud_Mask, and QF_DNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
